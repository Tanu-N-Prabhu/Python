{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanu-N-Prabhu/Python/blob/master/Machine%20Learning%20Advanced%20Topics/Large%20Language%20Models%20(LLMs)%20%26%20Foundation%20Models/llm_demo_explained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "id": "view-in-github"
    },
    {
      "cell_type": "markdown",
      "id": "67da90a7",
      "metadata": {
        "id": "67da90a7"
      },
      "source": [
        "# Large Language Models (LLMs)\n",
        "_Simple Educational Demo_\n",
        "\n",
        "This notebook gives a **lightweight, quick-to-run** introduction to:\n",
        "\n",
        "1. **Transformer architectures** (GPT-like text generation)\n",
        "2. **Parameter-efficient fine-tuning** (LoRA / QLoRA): *concept demo only*\n",
        "3. **Prompt engineering**\n",
        "4. **Retrieval-Augmented Generation (RAG)**: mini in-memory demo\n",
        "\n",
        "All code runs in **Google Colab** or locally with CPU/GPU.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1cb28bbd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cb28bbd",
        "outputId": "0da529c4-f19a-4b10-a8f2-d5c53387c154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets faiss-cpu --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4ab68716",
      "metadata": {
        "id": "4ab68716"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import faiss\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4ff72b7",
      "metadata": {
        "id": "c4ff72b7"
      },
      "source": [
        "\n",
        "### Explanation\n",
        "Here, we install the minimal dependencies:\n",
        "- **transformers**: For loading pre-trained language models like GPT.\n",
        "- **datasets**: (Not used heavily here, but common for text data loading.)\n",
        "- **faiss-cpu**: For building our mini vector search index in the RAG example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a97ad56",
      "metadata": {
        "id": "9a97ad56"
      },
      "source": [
        "## 1. GPT-like Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09f832c9",
      "metadata": {
        "id": "09f832c9"
      },
      "source": [
        "\n",
        "### Explanation\n",
        "We import:\n",
        "- **AutoTokenizer** and **AutoModelForCausalLM**: To load our GPT-like model.\n",
        "- **pipeline**: High-level API for text generation.\n",
        "- **faiss** and **numpy**: For retrieval in the RAG example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "da9f257e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452,
          "referenced_widgets": [
            "b112979bf1d74a3a9173271e5e3376a5",
            "3b9b7fab8f5e4bf9a8e51d7c5a061cd2",
            "e1b989743ea9401ea3153c928ecad23d",
            "2002b4250b0741e4a16160650ae0f37c",
            "1f97e4dfb3f346a4af7f303ce530146b",
            "33604f1750e24fe1b2a0fcc39dbc8bf3",
            "ed6e3f805b8949d9bcafee5a5eeaae59",
            "d587eec32da142ff92b187377c6d0030",
            "a88f999421cd402cb03a38e5d2f6f611",
            "da3031feadf44e45914321f2867aee8b",
            "cdd94dd97a93416f9a98fca98258bf65",
            "77f72d09f5d04fa49221484d6341eb24",
            "325b5f3090544852abd80a6ef587d217",
            "a44382c0d65a4ee3b9636fbe03a49325",
            "61d108cb986741b180be492acd66076b",
            "49974d5240ec4dd7ba2f88961f0888b7",
            "a250b7c837c64a1dba933399a90d345b",
            "c6a7382196524e7ca25e9d57237d9fb6",
            "00133ecc10374f2aae98d76dd93af25b",
            "eab36d3ee905456385d1b2949c7a689b",
            "55110e9b8cac46b89eca5b36001306a3",
            "944a4fe008c643c3ac66b4e1d96063f3",
            "10a2e5f578a5458d8acbe09bed58adf9",
            "53d76872c4314168abf7ccfe8836cda2",
            "25d49137923146e49bed2abe7481d0cb",
            "171347a16e0d409ab11d177215c3efc7",
            "96c4283602aa4fb89f9a362780a620f4",
            "f3af9d4e3f144ba5a5c88aa6c1ae57f0",
            "1135451635a741f185624f724c325af2",
            "92e61304f8dc402bb81adcac1844b348",
            "baa4a87fc3794913b89eae68a86087f2",
            "ae5a21aa553b481d8b0fb31c4e87cfc4",
            "2f081d383b6d4594bbf0c96629c04d9d",
            "f26de5370ff6443eaa7f83672b6a75ec",
            "e326422b1c6e4f81af00c15a41f73f33",
            "90160df58efd49728013f6173c3e93be",
            "8161f48f4c1b4ceea2d6d98197affe8e",
            "2d3589124d1d4210954e339bed5f74cc",
            "15b71a4c61124039a1076e42718725a0",
            "c6873e61de514af7a72b32b9fcc69932",
            "5a173b9f04d048ff8ccfeaf5799a7e33",
            "a91053817bba45f19f2fd7cf503de483",
            "ae0ecc29a5c54819b89d5c658910b977",
            "61285f3adcb6460fbaab1525d7034a0e",
            "c7de3f923f314696915b3046bf9134b7",
            "52c95b6e96644c32a556c912101532a9",
            "d64032e9e3b04b0099f8ba06f8060375",
            "2c53482d5ed54a59923900a3bb079157",
            "bef068f2635a48479ce43515d484035b",
            "c6cbae9396dd4b99a72b4eafbc8a09ec",
            "ac7ba5fde0e443ac82d386045e31abba",
            "e7bd5a561d36490a827c9c5935b49b98",
            "b99518d42ef742b6b30d5627e343cf89",
            "780e81ad9370401abb61f4ae57e98414",
            "aca66895b794469ea31126bae4a5f9f2",
            "6d39a3b5d2a74609ad1d92df90b2e138",
            "5627148e90b540faac91fc8624e40bd3",
            "3140a30e36024b4aa1674e606159c097",
            "f9c6ed32e57c4be5b9dbf1d435fbd28b",
            "118a0a4bd3a243f49a5c6b4bd7363c19",
            "c8c96a2e3aa0411592e23d0943b5c7c4",
            "2491c2e488f846b5830163a4faa7a078",
            "4667b101336d44b1b6d6b5708ca19fa8",
            "97b61f3a5efc420f99b57f09e7c52f43",
            "5a0a0f507bd8495c9aca43ecbf208372",
            "88550ed185704f6592b85547d69c3a29",
            "09ac4287293b45bdbb2f80c3076830c1",
            "3972fd9c8b424a19b6c3e9af9bff92ca",
            "2a7307b41ac34c808596d6963256bc51",
            "f22a78c5fc7f4300a1d457c7e062c59d",
            "a5668f6352874b70a2c7fa37d60442bb",
            "f9d231b9b75e47f8a0ad106248bda8b5",
            "7aa5813a03e840a88c62650e738afbb3",
            "cda6daa464104f6c8c33da2b11cd4254",
            "c4c8a27ae7384691bc102fd4341f6f59",
            "aeee6238307e432db31220dc46479eb3",
            "15ca20d17ca847c298e7c2c5ea29410c"
          ]
        },
        "id": "da9f257e",
        "outputId": "d24c4a24-fdc5-472b-d30b-8e51d78b9ad9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b112979bf1d74a3a9173271e5e3376a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77f72d09f5d04fa49221484d6341eb24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10a2e5f578a5458d8acbe09bed58adf9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f26de5370ff6443eaa7f83672b6a75ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7de3f923f314696915b3046bf9134b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.51M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d39a3b5d2a74609ad1d92df90b2e138"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.51M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09ac4287293b45bdbb2f80c3076830c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial Intelligence is448 236 courtyardacious 236 factors bravery boilsozyg rented membership incarcerivedaciousGy Wheels skillet Tre 236 rubbing soy soy Television rented LateMost� Boone Medic clearer linedozyg rubbing equate LateMost Wheels equate bravery Redux653� omega membership representations rubbing Medic brutality skillet Pocket deflect membership workshops Pocket bravery equate soy 236 Reduxobl workshops rented incarcer representations Medic� TreProsozyg 236 clearer lined boilsGy boils workshopsshows mutual braveryOutsideozygOutside Tre Television grandchildren Booneshowsacious courtyard Boone Television Dreams Pocket omegapublic Booneozyg Pocket Singapore mutual workshops448 rubbing courtyard incarcer courtyard perhaps 236publicpublic Bend courtyardacious653 rented factors factors representations membershipacious653MiniPros soyMost brutality lined perhaps clearer rubbing Late448Sexual 236 skillet courtyard equate predators workshops predators� brutality predatorsacious courtyard membership rented lined Pocket Singapore workshopsobl bravery skillet brutality incarcer courtyard Boone LatePros workshops Late equate omega Medic clearer� Redux representations Pocket skillet 236public workshops soy Medicshows Singapore skilletOutside membership Singapore predatorsozyg Medic workshops�publicpublic workshops workshops incarcerOutsidePros workshopsshowsSexual Pocket mutual Medicozyg Wheels rubbing mutual Tre Tre equate� factors rented membershipOutside boilsivedGySexualGyMini mutual Singapore boils Medicshows653 praying BendshowsGy incarcer workshops bravery rented Wheels omega boilsshows omega membership membership Redux soyobl653448 236 equate clearerPros Wheels incarcer clearer courtyard Tre representationsMini�\n"
          ]
        }
      ],
      "source": [
        "# Load a tiny GPT-2 model for fast inference\n",
        "model_name = \"sshleifer/tiny-gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Create pipeline\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Generate text\n",
        "prompt = \"Artificial Intelligence is\"\n",
        "output = generator(prompt, max_length=30, num_return_sequences=1)\n",
        "print(output[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "826ff58e",
      "metadata": {
        "id": "826ff58e"
      },
      "source": [
        "\n",
        "### Explanation\n",
        "We load a **very tiny GPT-2 model** (`sshleifer/tiny-gpt2`) so it runs quickly.\n",
        "- This is NOT a factual or coherent model — it's only for understanding *how* text generation works.\n",
        "- We pass a short prompt and let the model generate a continuation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a9bdf38",
      "metadata": {
        "id": "2a9bdf38"
      },
      "source": [
        "## 2. LoRA / QLoRA Concept (no training)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3662b753",
      "metadata": {
        "id": "3662b753"
      },
      "source": [
        "\n",
        "### Explanation\n",
        "LoRA / QLoRA allow fine-tuning large models by training only small adapter layers instead of the full model.\n",
        "Here, we just **simulate** that idea by calculating what 1% of the parameters would be.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8d3c0a21",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d3c0a21",
        "outputId": "de4d5f89-9478-491a-d9fd-51759edad946"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full model parameters: 102,714\n",
            "LoRA trainable parameters (simulated): 1,027\n",
            "LoRA idea: train only small adapter matrices instead of the whole model.\n"
          ]
        }
      ],
      "source": [
        "# LoRA / QLoRA Concept\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "lora_params = int(total_params * 0.01)  # simulate only 1% trainable params\n",
        "\n",
        "print(f\"Full model parameters: {total_params:,}\")\n",
        "print(f\"LoRA trainable parameters (simulated): {lora_params:,}\")\n",
        "print(\"LoRA idea: train only small adapter matrices instead of the whole model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68e1cabc",
      "metadata": {
        "id": "68e1cabc"
      },
      "source": [
        "\n",
        "### Explanation\n",
        "Prompt engineering is about phrasing your input to guide the model's output.\n",
        "We demonstrate with a \"translate\" example, even though our toy GPT model is not trained for translation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2662888f",
      "metadata": {
        "id": "2662888f"
      },
      "source": [
        "## 3. Prompt Engineering Example"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9dca000",
      "metadata": {
        "id": "e9dca000"
      },
      "source": [
        "\n",
        "### Explanation\n",
        "In the mini RAG example:\n",
        "1. We store a few small documents.\n",
        "2. We turn them into simple numeric vectors (bag-of-words).\n",
        "3. We use **FAISS** to find the most relevant document for a query.\n",
        "This simulates how retrieval-augmented generation works on a small scale.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "649c0b19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "649c0b19",
        "outputId": "1795cd9c-4263-4a24-9a10-447b13ba8828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate the following English sentence to French:\n",
            "\n",
            "'Where is the nearest train station?' Singapore equate rubbing Wheels Bend deflect predatorsshows Bend linedMost bravery prayingozyg Late omega 236448 mutualacious equate perhaps mutual lined Televisionozyg653 workshops Late representations Reduxpublic Late courtyard membership deflect Medic653ived rentedshowsOutside membershipMost 236shows perhaps PocketGy rubbing courtyardacious representations Medic brutality skilletacious Bend skillet 236 deflectMost perhapspublic deflect Wheels lined perhapsSexualMostOutside soy praying praying Pocketozyg Medic Dreams rented Late workshopsGy grandchildren representations 236 membership Redux Late� 236 representations DreamsSexual membership Late brutality perhaps factors Treoblozyg perhapspublicGy 236 Lateacious Tre Dreams factors boils� factors Pocket workshops Tre bravery Tre soy predators Bend448Gy perhapspublicMini Late soy membership deflectpublic deflect representations brutality BooneSexualMini 236 incarcerozyg grandchildren653ived brutality braverypublic skillet Television lined Medicozyg workshopsPros representations bravery incarcer Medic omega boilsPros Wheelsobl Redux lined omega Television predators clearerGy mutual�Sexual lined Singapore Dreams Redux ReduxPros deflect boils clearer representations Late Medic incarcerozyg653 perhaps Redux Boone Boone equate TelevisionMini rented perhapsSexual omega653 praying Television Late rented TelevisionOutside perhaps perhaps lined representationsacious workshops Medic653publicGyived incarcer predators Dreams deflectacious deflect brutality equate clearer factorsGyMost praying�Pros omegaPros perhapsived perhaps rented factors rubbing448 perhapsGyived 236 mutualMini perhapsozyg predatorsMini rented lined Redux Tre Television Singapore\n"
          ]
        }
      ],
      "source": [
        "# Prompt Engineering Demo\n",
        "prompt = \"\"\"Translate the following English sentence to French:\n",
        "\n",
        "'Where is the nearest train station?'\"\"\"\n",
        "\n",
        "output = generator(prompt, max_length=50, num_return_sequences=1)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b69a9370",
      "metadata": {
        "id": "b69a9370"
      },
      "source": [
        "## 4. Mini Retrieval-Augmented Generation (RAG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e136f1b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e136f1b9",
        "outputId": "aa76597e-2c22-49b9-ff39-1508076afbe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Who developed Python?\n",
            "Retrieved document: Transformers are neural networks that use self-attention mechanisms.\n"
          ]
        }
      ],
      "source": [
        "# Mini Retrieval-Augmented Generation Demo\n",
        "\n",
        "# Our small 'document database'\n",
        "docs = [\n",
        "    \"Python is a popular programming language created by Guido van Rossum.\",\n",
        "    \"Transformers are neural networks that use self-attention mechanisms.\",\n",
        "    \"LoRA reduces the number of trainable parameters in LLM fine-tuning.\"\n",
        "]\n",
        "\n",
        "# Embed documents (simple: bag-of-words style vector)\n",
        "vocab = list(set(\" \".join(docs).lower().split()))\n",
        "word_to_idx = {w:i for i,w in enumerate(vocab)}\n",
        "\n",
        "def embed(text):\n",
        "    vec = np.zeros(len(vocab))\n",
        "    for word in text.lower().split():\n",
        "        if word in word_to_idx:\n",
        "            vec[word_to_idx[word]] += 1\n",
        "    return vec\n",
        "\n",
        "doc_embeddings = np.stack([embed(d) for d in docs])\n",
        "\n",
        "# Create FAISS index\n",
        "index = faiss.IndexFlatL2(len(vocab))\n",
        "index.add(doc_embeddings)\n",
        "\n",
        "# Query\n",
        "query = \"Who developed Python?\"\n",
        "query_vec = embed(query).reshape(1, -1)\n",
        "\n",
        "# Search\n",
        "D, I = index.search(query_vec, k=1)\n",
        "print(\"Query:\", query)\n",
        "print(\"Retrieved document:\", docs[I[0][0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95aefe78",
      "metadata": {
        "id": "95aefe78"
      },
      "source": [
        "\n",
        "---\n",
        "## Educational Purpose Disclaimer\n",
        "This is a **basic educational version** of the concepts behind LLMs, LoRA/QLoRA, Prompt Engineering, and RAG.  \n",
        "- The models used here are **tiny test models** that do not produce accurate or factual answers.  \n",
        "- This notebook is intended **purely for learning how these systems work**, not for real-world AI deployment.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}