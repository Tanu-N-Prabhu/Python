{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMz0HWt1vcpiqSZmx2BeGIA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanu-N-Prabhu/Python/blob/master/Machine%20Learning%20Interview%20Prep%20Questions/Reinforcement%20Learning%20Algorithms/Deep%20Q-Network/Deep_Q_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Q-Network (DQN) Implementation from Scratch\n",
        "\n",
        "This notebook shows a **step-by-step implementation of a Deep Q-Network (DQN)** for solving the **CartPole environment**.  \n",
        "We avoid complex class structures and keep the code **simple and easy to understand**.\n"
      ],
      "metadata": {
        "id": "rkJP-By_9cqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# 1. Build the Q-Network (simple 2-layer NN)\n",
        "def build_model(state_dim, action_dim):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(state_dim, 24), nn.ReLU(),\n",
        "        nn.Linear(24, 24), nn.ReLU(),\n",
        "        nn.Linear(24, action_dim)\n",
        "    )\n",
        "\n",
        "# 2. Choose action (epsilon-greedy)\n",
        "def choose_action(state, epsilon, q_network, action_dim):\n",
        "    if random.random() < epsilon:\n",
        "        return random.randrange(action_dim)  # random action\n",
        "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "    q_values = q_network(state_tensor)\n",
        "    return torch.argmax(q_values).item()\n",
        "\n",
        "# 3. Training step (on one batch from memory)\n",
        "def train(q_network, target_network, memory, optimizer, gamma, batch_size=32):\n",
        "    if len(memory) < batch_size:\n",
        "        return\n",
        "    batch = random.sample(memory, batch_size)\n",
        "    states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "    states = torch.FloatTensor(np.array(states))\n",
        "    actions = torch.LongTensor(actions).unsqueeze(1)\n",
        "    rewards = torch.FloatTensor(rewards)\n",
        "    next_states = torch.FloatTensor(np.array(next_states))\n",
        "    dones = torch.FloatTensor(dones)\n",
        "\n",
        "    # Current Q values\n",
        "    q_values = q_network(states).gather(1, actions).squeeze()\n",
        "\n",
        "    # Next Q values (from target network)\n",
        "    next_q_values = target_network(next_states).max(1)[0]\n",
        "    target = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "    # Loss (MSE)\n",
        "    loss = nn.MSELoss()(q_values, target.detach())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# 4. Main Training Loop\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "q_network = build_model(state_dim, action_dim)\n",
        "target_network = build_model(state_dim, action_dim)\n",
        "target_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "optimizer = optim.Adam(q_network.parameters(), lr=0.001)\n",
        "\n",
        "# Hyperparameters\n",
        "episodes = 200\n",
        "gamma = 0.99\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "target_update = 10\n",
        "memory = []\n",
        "\n",
        "for ep in range(episodes):\n",
        "    state = env.reset()   # Gymnasium reset returns (state, info)\n",
        "    total_reward = 0\n",
        "\n",
        "    for t in range(200):\n",
        "        action = choose_action(state, epsilon, q_network, action_dim)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "\n",
        "        # Store in memory\n",
        "        memory.append((state, action, reward, next_state, done))\n",
        "        if len(memory) > 10000:\n",
        "            memory.pop(0)\n",
        "\n",
        "        # Train on random batch\n",
        "        train(q_network, target_network, memory, optimizer, gamma)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Decay epsilon\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "    # Update target network\n",
        "    if ep % target_update == 0:\n",
        "        target_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "    print(f\"Episode {ep}, Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8VeSXDBDkGo",
        "outputId": "06805312-e277-464e-a69c-7e6ca0065485"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Reward: 15.0, Epsilon: 0.99\n",
            "Episode 1, Reward: 18.0, Epsilon: 0.99\n",
            "Episode 2, Reward: 17.0, Epsilon: 0.99\n",
            "Episode 3, Reward: 20.0, Epsilon: 0.98\n",
            "Episode 4, Reward: 16.0, Epsilon: 0.98\n",
            "Episode 5, Reward: 25.0, Epsilon: 0.97\n",
            "Episode 6, Reward: 15.0, Epsilon: 0.97\n",
            "Episode 7, Reward: 15.0, Epsilon: 0.96\n",
            "Episode 8, Reward: 12.0, Epsilon: 0.96\n",
            "Episode 9, Reward: 25.0, Epsilon: 0.95\n",
            "Episode 10, Reward: 10.0, Epsilon: 0.95\n",
            "Episode 11, Reward: 13.0, Epsilon: 0.94\n",
            "Episode 12, Reward: 19.0, Epsilon: 0.94\n",
            "Episode 13, Reward: 12.0, Epsilon: 0.93\n",
            "Episode 14, Reward: 13.0, Epsilon: 0.93\n",
            "Episode 15, Reward: 21.0, Epsilon: 0.92\n",
            "Episode 16, Reward: 13.0, Epsilon: 0.92\n",
            "Episode 17, Reward: 16.0, Epsilon: 0.91\n",
            "Episode 18, Reward: 12.0, Epsilon: 0.91\n",
            "Episode 19, Reward: 19.0, Epsilon: 0.90\n",
            "Episode 20, Reward: 12.0, Epsilon: 0.90\n",
            "Episode 21, Reward: 9.0, Epsilon: 0.90\n",
            "Episode 22, Reward: 18.0, Epsilon: 0.89\n",
            "Episode 23, Reward: 9.0, Epsilon: 0.89\n",
            "Episode 24, Reward: 23.0, Epsilon: 0.88\n",
            "Episode 25, Reward: 62.0, Epsilon: 0.88\n",
            "Episode 26, Reward: 24.0, Epsilon: 0.87\n",
            "Episode 27, Reward: 51.0, Epsilon: 0.87\n",
            "Episode 28, Reward: 41.0, Epsilon: 0.86\n",
            "Episode 29, Reward: 11.0, Epsilon: 0.86\n",
            "Episode 30, Reward: 10.0, Epsilon: 0.86\n",
            "Episode 31, Reward: 13.0, Epsilon: 0.85\n",
            "Episode 32, Reward: 22.0, Epsilon: 0.85\n",
            "Episode 33, Reward: 37.0, Epsilon: 0.84\n",
            "Episode 34, Reward: 25.0, Epsilon: 0.84\n",
            "Episode 35, Reward: 29.0, Epsilon: 0.83\n",
            "Episode 36, Reward: 15.0, Epsilon: 0.83\n",
            "Episode 37, Reward: 13.0, Epsilon: 0.83\n",
            "Episode 38, Reward: 29.0, Epsilon: 0.82\n",
            "Episode 39, Reward: 13.0, Epsilon: 0.82\n",
            "Episode 40, Reward: 14.0, Epsilon: 0.81\n",
            "Episode 41, Reward: 32.0, Epsilon: 0.81\n",
            "Episode 42, Reward: 10.0, Epsilon: 0.81\n",
            "Episode 43, Reward: 13.0, Epsilon: 0.80\n",
            "Episode 44, Reward: 20.0, Epsilon: 0.80\n",
            "Episode 45, Reward: 17.0, Epsilon: 0.79\n",
            "Episode 46, Reward: 11.0, Epsilon: 0.79\n",
            "Episode 47, Reward: 17.0, Epsilon: 0.79\n",
            "Episode 48, Reward: 24.0, Epsilon: 0.78\n",
            "Episode 49, Reward: 40.0, Epsilon: 0.78\n",
            "Episode 50, Reward: 28.0, Epsilon: 0.77\n",
            "Episode 51, Reward: 60.0, Epsilon: 0.77\n",
            "Episode 52, Reward: 38.0, Epsilon: 0.77\n",
            "Episode 53, Reward: 33.0, Epsilon: 0.76\n",
            "Episode 54, Reward: 21.0, Epsilon: 0.76\n",
            "Episode 55, Reward: 17.0, Epsilon: 0.76\n",
            "Episode 56, Reward: 60.0, Epsilon: 0.75\n",
            "Episode 57, Reward: 49.0, Epsilon: 0.75\n",
            "Episode 58, Reward: 40.0, Epsilon: 0.74\n",
            "Episode 59, Reward: 14.0, Epsilon: 0.74\n",
            "Episode 60, Reward: 58.0, Epsilon: 0.74\n",
            "Episode 61, Reward: 42.0, Epsilon: 0.73\n",
            "Episode 62, Reward: 29.0, Epsilon: 0.73\n",
            "Episode 63, Reward: 14.0, Epsilon: 0.73\n",
            "Episode 64, Reward: 27.0, Epsilon: 0.72\n",
            "Episode 65, Reward: 13.0, Epsilon: 0.72\n",
            "Episode 66, Reward: 46.0, Epsilon: 0.71\n",
            "Episode 67, Reward: 31.0, Epsilon: 0.71\n",
            "Episode 68, Reward: 38.0, Epsilon: 0.71\n",
            "Episode 69, Reward: 25.0, Epsilon: 0.70\n",
            "Episode 70, Reward: 37.0, Epsilon: 0.70\n",
            "Episode 71, Reward: 24.0, Epsilon: 0.70\n",
            "Episode 72, Reward: 47.0, Epsilon: 0.69\n",
            "Episode 73, Reward: 31.0, Epsilon: 0.69\n",
            "Episode 74, Reward: 16.0, Epsilon: 0.69\n",
            "Episode 75, Reward: 16.0, Epsilon: 0.68\n",
            "Episode 76, Reward: 23.0, Epsilon: 0.68\n",
            "Episode 77, Reward: 22.0, Epsilon: 0.68\n",
            "Episode 78, Reward: 15.0, Epsilon: 0.67\n",
            "Episode 79, Reward: 48.0, Epsilon: 0.67\n",
            "Episode 80, Reward: 29.0, Epsilon: 0.67\n",
            "Episode 81, Reward: 17.0, Epsilon: 0.66\n",
            "Episode 82, Reward: 39.0, Epsilon: 0.66\n",
            "Episode 83, Reward: 15.0, Epsilon: 0.66\n",
            "Episode 84, Reward: 29.0, Epsilon: 0.65\n",
            "Episode 85, Reward: 48.0, Epsilon: 0.65\n",
            "Episode 86, Reward: 74.0, Epsilon: 0.65\n",
            "Episode 87, Reward: 14.0, Epsilon: 0.64\n",
            "Episode 88, Reward: 21.0, Epsilon: 0.64\n",
            "Episode 89, Reward: 53.0, Epsilon: 0.64\n",
            "Episode 90, Reward: 33.0, Epsilon: 0.63\n",
            "Episode 91, Reward: 41.0, Epsilon: 0.63\n",
            "Episode 92, Reward: 89.0, Epsilon: 0.63\n",
            "Episode 93, Reward: 102.0, Epsilon: 0.62\n",
            "Episode 94, Reward: 32.0, Epsilon: 0.62\n",
            "Episode 95, Reward: 50.0, Epsilon: 0.62\n",
            "Episode 96, Reward: 48.0, Epsilon: 0.61\n",
            "Episode 97, Reward: 50.0, Epsilon: 0.61\n",
            "Episode 98, Reward: 60.0, Epsilon: 0.61\n",
            "Episode 99, Reward: 25.0, Epsilon: 0.61\n",
            "Episode 100, Reward: 78.0, Epsilon: 0.60\n",
            "Episode 101, Reward: 34.0, Epsilon: 0.60\n",
            "Episode 102, Reward: 95.0, Epsilon: 0.60\n",
            "Episode 103, Reward: 12.0, Epsilon: 0.59\n",
            "Episode 104, Reward: 17.0, Epsilon: 0.59\n",
            "Episode 105, Reward: 44.0, Epsilon: 0.59\n",
            "Episode 106, Reward: 47.0, Epsilon: 0.58\n",
            "Episode 107, Reward: 108.0, Epsilon: 0.58\n",
            "Episode 108, Reward: 38.0, Epsilon: 0.58\n",
            "Episode 109, Reward: 35.0, Epsilon: 0.58\n",
            "Episode 110, Reward: 103.0, Epsilon: 0.57\n",
            "Episode 111, Reward: 164.0, Epsilon: 0.57\n",
            "Episode 112, Reward: 164.0, Epsilon: 0.57\n",
            "Episode 113, Reward: 118.0, Epsilon: 0.56\n",
            "Episode 114, Reward: 60.0, Epsilon: 0.56\n",
            "Episode 115, Reward: 109.0, Epsilon: 0.56\n",
            "Episode 116, Reward: 120.0, Epsilon: 0.56\n",
            "Episode 117, Reward: 18.0, Epsilon: 0.55\n",
            "Episode 118, Reward: 73.0, Epsilon: 0.55\n",
            "Episode 119, Reward: 49.0, Epsilon: 0.55\n",
            "Episode 120, Reward: 58.0, Epsilon: 0.55\n",
            "Episode 121, Reward: 191.0, Epsilon: 0.54\n",
            "Episode 122, Reward: 125.0, Epsilon: 0.54\n",
            "Episode 123, Reward: 28.0, Epsilon: 0.54\n",
            "Episode 124, Reward: 40.0, Epsilon: 0.53\n",
            "Episode 125, Reward: 192.0, Epsilon: 0.53\n",
            "Episode 126, Reward: 200.0, Epsilon: 0.53\n",
            "Episode 127, Reward: 42.0, Epsilon: 0.53\n",
            "Episode 128, Reward: 98.0, Epsilon: 0.52\n",
            "Episode 129, Reward: 13.0, Epsilon: 0.52\n",
            "Episode 130, Reward: 43.0, Epsilon: 0.52\n",
            "Episode 131, Reward: 200.0, Epsilon: 0.52\n",
            "Episode 132, Reward: 44.0, Epsilon: 0.51\n",
            "Episode 133, Reward: 187.0, Epsilon: 0.51\n",
            "Episode 134, Reward: 173.0, Epsilon: 0.51\n",
            "Episode 135, Reward: 70.0, Epsilon: 0.51\n",
            "Episode 136, Reward: 68.0, Epsilon: 0.50\n",
            "Episode 137, Reward: 43.0, Epsilon: 0.50\n",
            "Episode 138, Reward: 200.0, Epsilon: 0.50\n",
            "Episode 139, Reward: 200.0, Epsilon: 0.50\n",
            "Episode 140, Reward: 180.0, Epsilon: 0.49\n",
            "Episode 141, Reward: 155.0, Epsilon: 0.49\n",
            "Episode 142, Reward: 118.0, Epsilon: 0.49\n",
            "Episode 143, Reward: 64.0, Epsilon: 0.49\n",
            "Episode 144, Reward: 121.0, Epsilon: 0.48\n",
            "Episode 145, Reward: 165.0, Epsilon: 0.48\n",
            "Episode 146, Reward: 200.0, Epsilon: 0.48\n",
            "Episode 147, Reward: 141.0, Epsilon: 0.48\n",
            "Episode 148, Reward: 64.0, Epsilon: 0.47\n",
            "Episode 149, Reward: 158.0, Epsilon: 0.47\n",
            "Episode 150, Reward: 77.0, Epsilon: 0.47\n",
            "Episode 151, Reward: 145.0, Epsilon: 0.47\n",
            "Episode 152, Reward: 144.0, Epsilon: 0.46\n",
            "Episode 153, Reward: 191.0, Epsilon: 0.46\n",
            "Episode 154, Reward: 200.0, Epsilon: 0.46\n",
            "Episode 155, Reward: 200.0, Epsilon: 0.46\n",
            "Episode 156, Reward: 63.0, Epsilon: 0.46\n",
            "Episode 157, Reward: 200.0, Epsilon: 0.45\n",
            "Episode 158, Reward: 200.0, Epsilon: 0.45\n",
            "Episode 159, Reward: 157.0, Epsilon: 0.45\n",
            "Episode 160, Reward: 200.0, Epsilon: 0.45\n",
            "Episode 161, Reward: 200.0, Epsilon: 0.44\n",
            "Episode 162, Reward: 149.0, Epsilon: 0.44\n",
            "Episode 163, Reward: 42.0, Epsilon: 0.44\n",
            "Episode 164, Reward: 103.0, Epsilon: 0.44\n",
            "Episode 165, Reward: 152.0, Epsilon: 0.44\n",
            "Episode 166, Reward: 194.0, Epsilon: 0.43\n",
            "Episode 167, Reward: 191.0, Epsilon: 0.43\n",
            "Episode 168, Reward: 77.0, Epsilon: 0.43\n",
            "Episode 169, Reward: 182.0, Epsilon: 0.43\n",
            "Episode 170, Reward: 187.0, Epsilon: 0.42\n",
            "Episode 171, Reward: 128.0, Epsilon: 0.42\n",
            "Episode 172, Reward: 195.0, Epsilon: 0.42\n",
            "Episode 173, Reward: 103.0, Epsilon: 0.42\n",
            "Episode 174, Reward: 181.0, Epsilon: 0.42\n",
            "Episode 175, Reward: 137.0, Epsilon: 0.41\n",
            "Episode 176, Reward: 172.0, Epsilon: 0.41\n",
            "Episode 177, Reward: 82.0, Epsilon: 0.41\n",
            "Episode 178, Reward: 91.0, Epsilon: 0.41\n",
            "Episode 179, Reward: 125.0, Epsilon: 0.41\n",
            "Episode 180, Reward: 39.0, Epsilon: 0.40\n",
            "Episode 181, Reward: 51.0, Epsilon: 0.40\n",
            "Episode 182, Reward: 153.0, Epsilon: 0.40\n",
            "Episode 183, Reward: 176.0, Epsilon: 0.40\n",
            "Episode 184, Reward: 126.0, Epsilon: 0.40\n",
            "Episode 185, Reward: 197.0, Epsilon: 0.39\n",
            "Episode 186, Reward: 76.0, Epsilon: 0.39\n",
            "Episode 187, Reward: 168.0, Epsilon: 0.39\n",
            "Episode 188, Reward: 159.0, Epsilon: 0.39\n",
            "Episode 189, Reward: 200.0, Epsilon: 0.39\n",
            "Episode 190, Reward: 143.0, Epsilon: 0.38\n",
            "Episode 191, Reward: 130.0, Epsilon: 0.38\n",
            "Episode 192, Reward: 162.0, Epsilon: 0.38\n",
            "Episode 193, Reward: 195.0, Epsilon: 0.38\n",
            "Episode 194, Reward: 155.0, Epsilon: 0.38\n",
            "Episode 195, Reward: 199.0, Epsilon: 0.37\n",
            "Episode 196, Reward: 158.0, Epsilon: 0.37\n",
            "Episode 197, Reward: 164.0, Epsilon: 0.37\n",
            "Episode 198, Reward: 174.0, Epsilon: 0.37\n",
            "Episode 199, Reward: 134.0, Epsilon: 0.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation in Plain English\n",
        "\n",
        "1. Environment:\n",
        "CartPole game → balance the pole by moving left or right.\n",
        "\n",
        "2. Q-Network:\n",
        "Small neural net:\n",
        "\n",
        "    * Input = state (4 numbers)\n",
        "    * Output = Q-values for each action (2 numbers)\n",
        "\n",
        "3. Replay Memory:\n",
        "Stores past experiences → (state, action, reward, next_state, done)\n",
        "\n",
        "4. Epsilon-Greedy:\n",
        "Sometimes random, sometimes best action.\n",
        "\n",
        "5. Training:\n",
        "\n",
        "    * Predict Q(s, a)\n",
        "\n",
        "    * Compute target = reward + γ * max(Q(s’, a’))\n",
        "\n",
        "    * Update network with loss.\n",
        "\n",
        "6. Target Network:\n",
        "Copy of Q-network, updated slowly, makes learning stable."
      ],
      "metadata": {
        "id": "ocv4P8ayEOR8"
      }
    }
  ]
}