# ğŸ“˜ `q_learning.ipynb` â€“ Q-Learning from Scratch

This notebook demonstrates how to implement the **Q-Learning algorithm** from scratch using only NumPy and basic Python logicâ€”no machine learning libraries or classes. It is designed to help students and interviewees understand the foundational principles of reinforcement learning by **building a working model step by step**.

---

## ğŸ“Œ What is Q-Learning?

Q-Learning is a **model-free**, **off-policy**, **value-based** reinforcement learning algorithm. It teaches an agent how to act optimally in a Markov Decision Process (MDP) by learning the value of actions in statesâ€”without needing a model of the environment.

### ğŸ§  Bellman Equation (Q-Update Rule):
```
Q(s, a) â† Q(s, a) + Î± * [r + Î³ * max(Q(sâ€², aâ€²)) âˆ’ Q(s, a)]
```
- `s` = current state  
- `a` = action taken  
- `r` = reward received  
- `sâ€²` = next state  
- `Î±` = learning rate  
- `Î³` = discount factor  

---

## ğŸ§± Environment

- A **4Ã—4 grid** world is used.
- The agent starts at the top-left corner `(0, 0)`.
- The goal is to reach the bottom-right corner `(3, 3)` with the **maximum reward**.
- Actions = Up, Down, Left, Right
- Rewards:
  - +1 for reaching the goal  
  - âˆ’0.01 for every other step (to encourage faster solutions)

---

## ğŸ” Training Parameters

| Parameter         | Value          | Description |
|------------------|----------------|-------------|
| `episodes`       | 500            | Number of training episodes |
| `alpha`          | 0.1            | Learning rate |
| `gamma`          | 0.99           | Discount factor for future rewards |
| `epsilon`        | 1.0 â†’ 0.01     | Exploration rate (decays each episode) |

---

## âœ… What Youâ€™ll Learn

- How Q-learning works behind the scenes  
- Why the **Q-table** matters and how itâ€™s updated  
- Implementing Îµ-greedy exploration  
- Understanding the **trade-off between exploration and exploitation**  
- Visualizing the optimal policy using directional arrows  

---

## ğŸ§­ Output

At the end of training, the agent learns the **optimal action** at each position in the grid. You will see a map like:

```
â†’ â†’ â†’ â†“  
â†“ â†’ â†“ â†“  
â†’ â†’ â†“ â†“  
â†’ â†’ â†’ âœ…
```

---

## ğŸ“‚ Files

| File                          | Description |
|------------------------------|-------------|
| `q_learning.ipynb`           | Fully documented Q-Learning notebook with implementation from scratch |
| `README.md`                  | This file - explains theory, setup, and purpose |

---

## ğŸ“ Who Should Use This?

- Students preparing for ML/AI interviews  
- Beginners trying to understand reinforcement learning  
- Educators building curriculum materials  
- Developers exploring agent-based AI  
