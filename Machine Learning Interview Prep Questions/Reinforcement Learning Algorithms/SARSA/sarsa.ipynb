{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+eCsb2DPyCxN+Zcur5U1u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanu-N-Prabhu/Python/blob/master/Machine%20Learning%20Interview%20Prep%20Questions/Reinforcement%20Learning%20Algorithms/SARSA/sarsa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SARSA: On-Policy Temporal Difference Control\n",
        "\n",
        "SARSA is a reinforcement learning algorithm used to learn the optimal action-value function \\( Q(s, a) \\) for a given environment. Unlike Q-Learning, which is off-policy, SARSA is **on-policy**, meaning it updates its Q-values using the action actually taken by the current policy.\n",
        "\n",
        "---\n",
        "\n",
        "## SARSA Update Rule\n",
        "\n",
        "The SARSA update rule is:\n",
        "\n",
        "$$[\n",
        "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right]\n",
        "]$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $$( s_t )$$: current state  \n",
        "- $$( a_t )$$: current action  \n",
        "- $$( r_{t+1} )$$: reward after taking action $$( a_t )$$  \n",
        "\n",
        "- $$( s_{t+1} )$$: next state  \n",
        "- $$( a_{t+1} )$$: next action taken by the current policy  \n",
        "- $$( \\alpha )$$: learning rate  \n",
        "- $$( \\gamma )$$: discount factor  \n",
        "\n",
        "---\n",
        "\n",
        "## Key Properties\n",
        "\n",
        "- **On-policy:** Learns from actions taken using its current policy.\n",
        "- **Exploration Strategy:** Uses Îµ-greedy to balance exploration and exploitation.\n",
        "- **Goal:** Learn an optimal policy to maximize expected cumulative rewards.\n",
        "\n",
        "---\n",
        "\n",
        "## Code Implementation (SARSA with FrozenLake)\n"
      ],
      "metadata": {
        "id": "y6NsEMuNuqjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Define the 4x4 GridWorld environment\n",
        "grid_size = 4\n",
        "num_states = grid_size * grid_size\n",
        "num_actions = 4  # 0 = Up, 1 = Right, 2 = Down, 3 = Left\n",
        "\n",
        "# Define the environment dynamics\n",
        "def get_next_state(state, action):\n",
        "    row = state // grid_size\n",
        "    col = state % grid_size\n",
        "\n",
        "    if action == 0 and row > 0:            # Up\n",
        "        row -= 1\n",
        "    elif action == 1 and col < grid_size-1:  # Right\n",
        "        col += 1\n",
        "    elif action == 2 and row < grid_size-1:  # Down\n",
        "        row += 1\n",
        "    elif action == 3 and col > 0:           # Left\n",
        "        col -= 1\n",
        "\n",
        "    return row * grid_size + col\n",
        "\n",
        "# Define the reward function\n",
        "def get_reward(state):\n",
        "    if state == 15:\n",
        "        return 10  # Goal state\n",
        "    else:\n",
        "        return -1  # Penalty for each step\n",
        "\n",
        "# Epsilon-greedy policy\n",
        "def choose_action(state, Q, epsilon):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        return random.randint(0, num_actions - 1)\n",
        "    else:\n",
        "        return np.argmax(Q[state])\n",
        "\n",
        "# SARSA Parameters\n",
        "alpha = 0.1       # Learning rate\n",
        "gamma = 0.99      # Discount factor\n",
        "epsilon = 0.1     # Exploration rate\n",
        "episodes = 500    # Total training episodes\n",
        "\n",
        "# Initialize Q-table with zeros\n",
        "Q = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Training Loop\n",
        "for episode in range(episodes):\n",
        "    state = 0  # Start at top-left corner (state 0)\n",
        "    action = choose_action(state, Q, epsilon)\n",
        "\n",
        "    while state != 15:  # Until we reach the goal\n",
        "        next_state = get_next_state(state, action)\n",
        "        reward = get_reward(next_state)\n",
        "        next_action = choose_action(next_state, Q, epsilon)\n",
        "\n",
        "        # SARSA Update\n",
        "        Q[state, action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n",
        "\n",
        "        # Move to next state-action\n",
        "        state = next_state\n",
        "        action = next_action\n",
        "\n",
        "# Display final policy\n",
        "actions_map = ['â†‘', 'â†’', 'â†“', 'â†']\n",
        "policy_grid = np.array([actions_map[np.argmax(Q[s])] if s != 15 else 'ðŸ' for s in range(num_states)]).reshape((grid_size, grid_size))\n",
        "print(\"Learned Policy (SARSA):\")\n",
        "print(policy_grid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXWypA5ivIdP",
        "outputId": "bad55886-1a96-43bc-b4f4-e1289d067f1e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Policy (SARSA):\n",
            "[['â†’' 'â†“' 'â†“' 'â†“']\n",
            " ['â†’' 'â†’' 'â†“' 'â†“']\n",
            " ['â†’' 'â†’' 'â†’' 'â†“']\n",
            " ['â†’' 'â†’' 'â†’' 'ðŸ']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Key Points\n",
        "* On-policy: SARSA follows the policy used to pick actions.\n",
        "\n",
        "* Exploration-aware: It learns based on the action actually taken.\n",
        "\n",
        "* Simple environment: 4x4 grid helps visualize learning steps clearly."
      ],
      "metadata": {
        "id": "UMlkniEWw-3v"
      }
    }
  ]
}